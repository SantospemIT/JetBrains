{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SantospemIT/JetBrains/blob/master/pipelines_preprocesamiento_ia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los pipelines para preprocesamiento son una herramienta poderosa en aprendizaje automático que permite encadenar varias operaciones de preprocesamiento de datos de forma secuencial. Estos pipelines pueden incluir transformaciones como:\n",
        "\n",
        "Normalización o estandarización de características: Asegura que todas las características tengan la misma escala, lo que es crucial para muchos algoritmos de aprendizaje automático.\n",
        "\n",
        "Imputación de valores faltantes: Rellena los valores faltantes en los datos con algún valor apropiado, como la media o la mediana de la característica.\n",
        "\n",
        "Codificación de variables categóricas: Convierte variables categóricas en representaciones numéricas que puedan ser utilizadas por algoritmos de aprendizaje automático.\n",
        "\n",
        "Selección de características: Selecciona un subconjunto relevante de características para el modelo.\n",
        "\n",
        "Reducción de dimensionalidad: Reduce la dimensionalidad de los datos manteniendo la mayor cantidad posible de información.\n",
        "\n",
        "La ventaja clave de utilizar pipelines es que permite encapsular todo el flujo de preprocesamiento en un solo objeto, lo que facilita la replicación del flujo en nuevos conjuntos de datos y la automatización de tareas. Además, los pipelines garantizan que las transformaciones se apliquen de manera coherente y en el orden correcto.\n",
        "\n",
        "Algunas bibliotecas populares en Python para trabajar con pipelines son scikit-learn y TensorFlow. Estas bibliotecas proporcionan clases y funciones específicas para construir y gestionar pipelines de preprocesamiento de datos."
      ],
      "metadata": {
        "id": "gyHGF_gdBfOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejemplo:**\n",
        "Claro, aquí tienes un ejemplo básico de cómo se puede usar Pipeline de scikit-learn en Python para encadenar varias etapas de preprocesamiento de datos y entrenar un modelo de regresión lineal"
      ],
      "metadata": {
        "id": "_5M4FMF-Bq1H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYNNa7EeAq05",
        "outputId": "48f7b59c-0da0-43ba-99c7-f88522cbb16d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error cuadrático medio: 56.25000000000006\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Supongamos que tienes un conjunto de datos con algunas características (X) y sus etiquetas correspondientes (y)\n",
        "X = [[1, 2, np.nan], [3, np.nan, 4], [5, 6, 7]]\n",
        "y = [10, 20, 30]\n",
        "\n",
        "# Dividimos los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creamos un pipeline que consta de dos etapas: imputación de valores faltantes y estandarización de características\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Completa los valores faltantes con la media de la columna\n",
        "    ('scaler', StandardScaler())  # Estandariza las características para que tengan media cero y varianza unitaria\n",
        "])\n",
        "\n",
        "# Aplicamos el pipeline al conjunto de entrenamiento y lo transformamos\n",
        "X_train_transformed = pipeline.fit_transform(X_train)\n",
        "\n",
        "# Entrenamos un modelo de regresión lineal usando los datos transformados\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_transformed, y_train)\n",
        "\n",
        "# Evaluamos el modelo en el conjunto de prueba\n",
        "X_test_transformed = pipeline.transform(X_test)\n",
        "y_pred = model.predict(X_test_transformed)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Error cuadrático medio:\", mse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejemplo:\n",
        "\n",
        "* Creamos un pipeline con dos etapas: SimpleImputer para imputar valores faltantes y StandardScaler para estandarizar las características.\n",
        "\n",
        "* Aplicamos el pipeline al conjunto de entrenamiento usando fit_transform.\n",
        "\n",
        "* Entrenamos un modelo de regresión lineal con los datos transformados.\n",
        "\n",
        "* Evaluamos el modelo en el conjunto de prueba usando el error cuadrático medio como métrica de evaluación.\n",
        "\n",
        "Este es solo un ejemplo básico de cómo usar Pipeline en scikit-learn. Dependiendo de los requisitos específicos de tu problema, puedes personalizar y expandir el pipeline con diferentes transformaciones y modelos."
      ],
      "metadata": {
        "id": "SZRr9n-oBzhX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t2si0xkwB9r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo:\n",
        "\n",
        "* Usamos ColumnTransformer para aplicar diferentes transformaciones a las características numéricas y categóricas.\n",
        "\n",
        "* Creamos un pipeline que incluye el preprocesamiento de datos y un clasificador RandomForest.\n",
        "\n",
        "* Dividimos los datos en conjuntos de entrenamiento y prueba.\n",
        "\n",
        "* Entrenamos el pipeline en los datos de entrenamiento y evaluamos su precisión en los datos de prueba.\n",
        "\n",
        "Este ejemplo ilustra cómo puedes construir un pipeline más complejo con diferentes transformaciones y modelos en un problema de clasificación."
      ],
      "metadata": {
        "id": "m7VwBET9B-Bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Supongamos que tenemos un conjunto de datos con características numéricas y categóricas, y la variable objetivo es binaria\n",
        "data = pd.read_csv('datos.csv')\n",
        "\n",
        "# Dividimos los datos en características (X) y la variable objetivo (y)\n",
        "X = data.drop(columns=['target'])\n",
        "y = data['target']\n",
        "\n",
        "# Definimos las columnas numéricas y categóricas\n",
        "numeric_features = X.select_dtypes(include=['int', 'float']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Creamos transformadores para las características numéricas y categóricas\n",
        "numeric_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Imputa valores faltantes con la media\n",
        "    ('scaler', StandardScaler())  # Estandariza las características\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Imputa valores faltantes con la moda\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Codifica las características categóricas en vectores one-hot\n",
        "])\n",
        "\n",
        "# Combinamos los transformadores utilizando ColumnTransformer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('cat', categorical_transformer, categorical_features)\n",
        "])\n",
        "\n",
        "# Creamos el pipeline completo con el preprocesamiento y un clasificador RandomForest\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# Dividimos los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entrenamos el pipeline en los datos de entrenamiento\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluamos el pipeline en los datos de prueba\n",
        "y_pred = pipeline.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Precisión del modelo:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "h-ICPf4gBy2J",
        "outputId": "51a529ef-38b0-4433-c45c-1711294febc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'datos.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1da8075e393e>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Supongamos que tenemos un conjunto de datos con características numéricas y categóricas, y la variable objetivo es binaria\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datos.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Dividimos los datos en características (X) y la variable objetivo (y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datos.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problema de aplicación:**\n",
        "\n",
        "**Problema: Predicción de precios de viviendas**\n",
        "\n",
        "Supongamos que trabajas para una empresa de bienes raíces y deseas construir un modelo para predecir los precios de las viviendas en función de varias características. Tienes un conjunto de datos que incluye características como el tamaño de la vivienda, el número de habitaciones, la ubicación, el año de construcción, etc.\n",
        "\n",
        "El conjunto de datos contiene una combinación de características numéricas y categóricas, así como algunos valores faltantes en algunas características.\n",
        "\n",
        "**Objetivo:**\n",
        "\n",
        "Construir un modelo predictivo que pueda predecir el precio de una vivienda en función de sus características.\n",
        "\n",
        "**Pasos a seguir:**\n",
        "\n",
        "\n",
        "* Preprocesamiento de datos: manejar los valores faltantes: imputar valores faltantes en características numéricas utilizando la media y en características categóricas utilizando la moda.\n",
        "Codificar características categóricas: utilizar codificación one-hot para convertir características categóricas en variables binarias.\n",
        "Estandarización: estandarizar características numéricas para que tengan una media de 0 y una desviación estándar de 1.\n",
        "Entrenamiento del modelo:\n",
        "\n",
        "* Dividir los datos en conjuntos de entrenamiento y prueba.\n",
        "Entrenar un modelo de regresión (por ejemplo, regresión lineal, árboles de decisión, random forest) en los datos de entrenamiento.\n",
        "\n",
        "* Evaluación del modelo: Evaluar el rendimiento del modelo en los datos de prueba utilizando métricas como el error cuadrático medio (MSE), el coeficiente de determinación (R^2), etc.\n",
        "Ajustar hiperparámetros del modelo si es necesario para mejorar el rendimiento.\n",
        "\n",
        "* Despliegue del modelo: Implementar el modelo entrenado en producción para que pueda ser utilizado para hacer predicciones sobre el precio de las viviendas nuevas.\n",
        "\n",
        "* Beneficios: Al construir un modelo predictivo, la empresa de bienes raíces puede ayudar a los clientes a determinar el precio de venta adecuado para sus propiedades.\n",
        "\n",
        "También puede proporcionar información valiosa sobre qué características influyen más en el precio de una vivienda, lo que puede ser útil para los agentes inmobiliarios y los compradores potenciales."
      ],
      "metadata": {
        "id": "f4WCFbzoCZTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Una posible solución\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cargar los datos\n",
        "url='https://raw.githubusercontent.com/jofsanchezci/Datos_Explo_IA/main/datos_viviendas.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Dividir datos en características (X) y etiquetas (y)\n",
        "X = data.drop(columns=['PrecioVenta'])\n",
        "y = data['PrecioVenta']\n",
        "\n",
        "# Dividir datos en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocesamiento de datos\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Construir pipeline\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('regressor', RandomForestRegressor())])\n",
        "\n",
        "# Entrenar modelo\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predicciones en el conjunto de prueba\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluar modelo\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f'MSE: {mse}')\n",
        "print(f'R^2: {r2}')\n",
        "\n",
        "# Visualizar importancia de características\n",
        "feat_importances = pipeline.named_steps['regressor'].feature_importances_\n",
        "feature_names = numeric_features.tolist() + \\\n",
        "    list(pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names(input_features=categorical_features))\n",
        "\n",
        "#sorted_idx = np.argsort(feat_importances)[-10:]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(range(len(sorted_idx)), feat_importances[sorted_idx], align='center')\n",
        "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
        "plt.xlabel('Importancia relativa')\n",
        "plt.title('Importancia de características')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "n-ocAYycC9c8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}